{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "pdf_files = glob.glob(path + \"/Protocolos/*.pdf\")\n",
    "print(len(pdf_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_acentos(texto):\n",
    "    acentos = { 'á': 'a', 'Á': 'a', 'é': 'e', 'É': 'e', 'í': 'i', \n",
    "               'Í': 'i', 'ó': 'o', 'Ó': 'o', 'ú': 'u', 'Ú': 'u', \n",
    "               'ü': 'u', 'Ü': 'u', 'ñ': 'n', 'Ñ': 'n'\n",
    "    }\n",
    "    for acento, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acento, sin_acento)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_caracteres_especiales(texto):\n",
    "        texto = re.sub(r'[•◦○◘◙◈◇◆◄►▲▼▶◀●➢]', '', texto)\n",
    "        texto = re.sub(r' +', ' ', texto)\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(path):\n",
    "    pdf_file = open(path, 'rb')\n",
    "    read_pdf = PyPDF2.PdfReader(pdf_file)\n",
    "    number_of_pages = len(read_pdf.pages)\n",
    "    page_content = \"\"\n",
    "    for page_number in range(number_of_pages):\n",
    "        page = read_pdf.pages[page_number]\n",
    "        page_content += page.extract_text()\n",
    "    pdf_file.close()\n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso 1\n",
      "355/422\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting cache for 0 1743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422\r"
     ]
    }
   ],
   "source": [
    "\n",
    "text = []\n",
    "\n",
    "print(\"Proceso 1\")\n",
    "index=1\n",
    "ttborrar = []\n",
    "for i in pdf_files:\n",
    "    # try:\n",
    "    print(f\"{index}/{len(pdf_files)}\", end='\\r')\n",
    "    text.append(convert_pdf_to_txt(i))\n",
    "    index+=1\n",
    "    # except:\n",
    "    #     print(\"Error\")\n",
    "    #     ttborrar.append(i)\n",
    "    #     index+=1\n",
    "    #     # text.append(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "profesores = pd.read_excel(\"profes.xlsx\")\n",
    "profesores[\"PROF\"] = profesores[\"PROF\"].apply(lambda x: re.sub(r' +', ' ', x))\n",
    "profesores['PROF'] = profesores['PROF'].apply(lambda x: x.strip())\n",
    "profesores.to_excel(\"profesores.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_title(documento):\n",
    "    if re.search(r\"Trabajo\\s*Terminal\\s*No\\.?\", documento, flags = re.IGNORECASE):\n",
    "        titulo = re.split(r'(?:T\\s*r\\s*a\\s*b\\s*a\\s*j\\s*o)\\s*(?:T\\s*e\\s*r\\s*m\\s*i\\s*n\\s*a\\s*l)?\\s*(?:No)\\s*\\.?', documento, flags = re.IGNORECASE | re.MULTILINE)[0]\n",
    "    else:\n",
    "        titulo = re.split(r'(?:T\\s*r\\s*a\\s*b\\s*a\\s*j\\s*o)\\s*(?:T\\s*e\\s*r\\s*m\\s*i\\s*n\\s*a\\s*l)?\\s*\\.?', documento, flags = re.IGNORECASE | re.MULTILINE)[0]\n",
    "    titulo = re.sub(r'\\n|^\\s*[^a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]+|\\s{2,}|(^\\s+|\\s+$)', ' ', titulo).strip()\n",
    "    return titulo\n",
    "\n",
    "def get_autores(documento):\n",
    "    if re.search(r'(?:(?:estudiante|alumno|alumna|integrante)s?\\s*:)', documento, flags = re.IGNORECASE):\n",
    "        documento = re.split(r'(?:(?:estudiante|alumno|alumna|integrante)s?\\s*:)', documento, maxsplit=1, flags = re.IGNORECASE | re.MULTILINE)[1]\n",
    "    else:\n",
    "        documento = re.split(r'(?:estudiantes?\\s*:?|alumn[oa]?s?\\s*:?|integrantes?\\s*:?)', documento, maxsplit=1, flags = re.IGNORECASE | re.MULTILINE)[1]\n",
    "    if re.search(r\"Directo\", documento, flags = re.IGNORECASE):\n",
    "        autores = re.split(r'Directo', documento, maxsplit=1, flags=re.IGNORECASE)[0]\n",
    "    else:\n",
    "        autores = re.split(r'resumen', documento, maxsplit=1, flags=re.IGNORECASE)[0]\n",
    "    if re.search(r\"e-?\\s*mail:?\", autores, flags=re.IGNORECASE):\n",
    "        autores = re.split(r\"e-?\\s*mail:?\", autores, maxsplit=1, flags=re.IGNORECASE)[0]\n",
    "    autores = re.sub(r's:', '', autores, flags=re.IGNORECASE)\n",
    "    autores = re.sub(r'\\n|^\\s*[^a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]+|\\s{2,}|(^\\s+|\\s+$)', ' ', autores).strip()\n",
    "    autores = re.sub(r'\\*|\\d', '', autores)\n",
    "    autores = re.sub(r'^[a-záéíóúüñ]', lambda x: x.group(0).upper(), autores)\n",
    "    autores = re.sub(r'\\s{2,}', ' ', autores)\n",
    "    autores = re.sub(r'\\.', '', autores)\n",
    "    autores = re.sub(r'\\[|\\]', '', autores)\n",
    "    # si hay espacios entre dos letras minusculas, se junta\n",
    "    autores = re.sub(r'(?<=[a-xzáéíóúüñ])\\s(?=[a-xzáéíóúüñ])', '', autores)\n",
    "    # si hay espacios entre una letra mayuscula y una minuscula, se junta\n",
    "    autores = re.sub(r'(?<=[A-ZÁÉÍÓÚÜÑ])\\s(?=[a-xzáéíóúüñ])', '', autores)\n",
    "    autores = [autor.strip() for autor in re.split(',|;|\\s+y\\s+', autores)]\n",
    "    # si hay elementos vacios en la lista, se eliminan\n",
    "    autores = [autor for autor in autores if autor != '']\n",
    "    return autores\n",
    "\n",
    "def get_director(documento):\n",
    "    try:\n",
    "        if re.search(r\"d\\s*i\\s*r\\s*e\\s*c\\s*t\\s*o\\s*r\\s*(?:e\\s*s|a\\s*s|a)?\\s*:\", documento, flags = re.IGNORECASE):\n",
    "            documento = re.split(r\"d\\s*i\\s*r\\s*e\\s*c\\s*t\\s*o\\s*r\\s*(?:e\\s*s|a\\s*s|a)?\\s*:\", documento, maxsplit=1, flags=re.IGNORECASE | re.MULTILINE)[1]\n",
    "        else:\n",
    "            documento = re.split(r\"d\\s*i\\s*r\\s*e\\s*c\\s*t\\s*o\\s*r\\s*(?:e\\s*s|a\\s*s|a)?\\s*:?\", documento, maxsplit=1, flags=re.IGNORECASE | re.MULTILINE)[1]\n",
    "        documento = re.split(r'Resumen|e\\s*[-]*\\s*mail|turno', documento, maxsplit=1, flags=re.IGNORECASE | re.MULTILINE)[0]\n",
    "        documento = re.sub(r'\\n|(\\*|(\\s{2,})|(^[^a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]+)|(\\s+$)|(\\w+@[^@]+\\.[a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]{2,}))|\\s*(?:M|Dra?)\\.?\\s*(?:en)?\\s*(?:ing|c)?\\.?', lambda m: ' ' if m.group(1) else '', documento, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        documento = re.sub(r'\\.', '', documento)\n",
    "        # si hay espacios entre dos letras minusculas, se junta\n",
    "        documento = re.sub(r'(?<=[a-xzáéíóúüñ])\\s(?=[a-xzáéíóúüñ])', '', documento)\n",
    "        # si hay espacios entre una letra mayuscula y una minuscula, se junta\n",
    "        documento = re.sub(r'(?<=[A-ZÁÉÍÓÚÜÑ])\\s(?=[a-xzáéíóúüñ])', '', documento)\n",
    "        documento = [i.strip() for i in re.split(',|;|\\s+y\\s+', documento)]\n",
    "        documento = [re.sub(r'^[a-záéíóúüñ]', lambda x: x.group(0).upper(), i) for i in documento]\n",
    "        directores = [director for director in documento if director != '']\n",
    "        for i, director in enumerate(directores):\n",
    "            if len(director.split(' ')) > 6:\n",
    "                directores[i] = ' '.join(director.split(' ')[:len(director.split(' '))//2])\n",
    "        vectorizer = CountVectorizer()\n",
    "        for i, director in enumerate(directores):\n",
    "            auxprofe = director\n",
    "            auxval=0\n",
    "            for profesor in profesores['PROF']:\n",
    "                # quitamos acentos \n",
    "                director_aux = quitar_acentos(director)\n",
    "                profesor_aux = quitar_acentos(profesor)\n",
    "\n",
    "                vectorizer.fit([director_aux, profesor_aux])\n",
    "                vector = vectorizer.transform([director_aux, profesor_aux])\n",
    "                if cosine_similarity(vector)[0][1] > 0.8:\n",
    "                    auxval = cosine_similarity(vector)[0][1]\n",
    "                    auxprofe = profesor\n",
    "                    break\n",
    "                if cosine_similarity(vector)[0][1] > 0.4:\n",
    "                    if cosine_similarity(vector)[0][1] > auxval:\n",
    "                        auxval = cosine_similarity(vector)[0][1]\n",
    "                        auxprofe = profesor\n",
    "            if auxval == 0:\n",
    "                # print(director_aux, \" / \",auxprofe, \" / \", auxval)\n",
    "                directores[i] = '¡¡¡¡¡¡¡¡¡¡¡¡¡REVISAR!!!!!!!!!!!!!!!' + director\n",
    "            else:\n",
    "                # print(director, \" / \",auxprofe, \" / \", auxval)\n",
    "                directores[i] = auxprofe      \n",
    "        documento = directores\n",
    "    except:\n",
    "        documento = ['']\n",
    "    finally:\n",
    "        return documento\n",
    "\n",
    "def get_ttnum(documento, ruta):\n",
    "    documento = re.split(r'(?:T\\s*r\\s*a\\s*b\\s*a\\s*j\\s*o)\\s*(?:T\\s*e\\s*r\\s*m\\s*i\\s*n\\s*a\\s*l)\\s*[No]*\\s*\\.?', documento, maxsplit=1, flags = re.IGNORECASE | re.MULTILINE)[1]\n",
    "    documento = re.split(r'(?:alumn[oa]?s?\\s*:?\\s*|integrantes\\s*:?\\s*)', documento, flags = re.IGNORECASE)[0]\n",
    "    documento = re.sub(r'\\s|No\\W*', '', documento)\n",
    "    documento = documento.replace('_', '')\n",
    "    documento = re.sub(r\"[^a-zA-Z0-9]\",\"\",documento)    \n",
    "    if re.search(r'^\\d{4}[a-zA-Z]\\d{3}$', documento):\n",
    "        documento = documento[:4] + '-' + documento[4:]\n",
    "    if not re.match(r'^\\d{4}-[a-zA-Z]\\d{3}$', documento):\n",
    "        documento = re.split(r'\\\\', ruta)[-1]\n",
    "        documento = re.split(r'\\.', documento)[0]\n",
    "        documento = re.sub(r\"[^a-zA-Z0-9]\",\"\",documento)\n",
    "        if re.search(r'^\\d{4}[a-zA-Z]\\d{3}$', documento):\n",
    "            documento = documento[:4] + '-' + documento[4:]\n",
    "        if not re.match(r'^\\d{4}-[a-zA-Z]\\d{3}$', documento):\n",
    "            documento = '-'\n",
    "    return documento.upper()\n",
    "\n",
    "def get_abstract(documento):\n",
    "    documento = re.split(r\"Resumen\", documento, maxsplit=1, flags = re.IGNORECASE | re.MULTILINE)[1]\n",
    "    documento = re.split(r\"Palabras?\\s*clave\", documento, maxsplit=1, flags = re.IGNORECASE | re.MULTILINE)[0]\n",
    "    documento = re.sub(r'\\n|^\\s*[^a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]+|\\s{2,}|(^\\s+|\\s+$)', ' ', documento).strip()\n",
    "    # eliminar todo lo que haya antes de la primera letra\n",
    "    documento = re.sub(r'^[^a-zA-Z]+', '', documento)\n",
    "    # if len(documento.split(' ')) > 400:\n",
    "        # documento = '-'\n",
    "    return documento\n",
    "\n",
    "def get_keywords(documento):\n",
    "    documento = re.split(r\"Palabra[s]?\\s*Clave[s]?\\s*[-\\._:]*\\s*\", documento, maxsplit=1, flags=re.IGNORECASE)[1]\n",
    "    documento = re.split(r\"\\.\", documento, maxsplit=1, flags=re.IGNORECASE)[0]\n",
    "    documento = re.sub(r'^[^a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]+|\\n|\\s{2,}', ' ', documento).strip()\n",
    "    keywords = [keyword.strip() for keyword in re.split(r\",\", documento) if len(keyword.strip()) < 30]\n",
    "    keywords = [re.sub(r'^[a-z]', lambda x: x.group(0).upper(), keyword) for keyword in keywords]\n",
    "    return keywords\n",
    "    \n",
    "def get_objetivo(documento):\n",
    "    i = 1\n",
    "    objetivoGeneral = ''\n",
    "    while len(objetivoGeneral) < 55:\n",
    "        objetivo = re.split(r\"[1-9]\\.?\\s*[-]?\\s*(?:O\\s*b\\s*j\\s*e\\s*t\\s*i\\s*v\\s*o)\", documento, maxsplit=i, flags=re.IGNORECASE)[i]\n",
    "        objetivo = re.split(r\"[2-9]\\.?\\s*[-]?\\s*(?:J\\s*u\\s*s\\s*t\\s*i\\s*f\\s*i\\s*c\\s*a\\s*c\\s*i\\s*[oó]\\s*n)\", objetivo, maxsplit=1, flags=re.IGNORECASE)[0]\n",
    "        objetivo = re.sub(r'\\n|\\s{2,}', ' ', objetivo).strip()\n",
    "\n",
    "        if re.search(r\"(?:objetivos?\\s*)?(?:especí?i?ficos|particulare?s?|secundarios?)\", objetivo, flags=re.IGNORECASE):\n",
    "            objetivoGeneral, objetivosEspecificos = re.split(r\"(?:objetivos?\\s*)?(?:especí?i?ficos|particulare?s?|secundarios?)\", objetivo, maxsplit=1, flags=re.IGNORECASE)\n",
    "        else:\n",
    "            objetivosEspecificos = '-'\n",
    "            objetivoGeneral = objetivo\n",
    "\n",
    "        if len(objetivoGeneral) < 50:\n",
    "            i += 1\n",
    "\n",
    "    objetivoGeneral = re.sub(r'^\\s*(?:objetivos? general|objetivo|general)\\s*', '', objetivoGeneral, flags=re.IGNORECASE)\n",
    "    objetivoGeneral = re.sub(r'^[^a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]+', '', objetivoGeneral)\n",
    "    objetivosEspecificos = re.sub(r'^[^a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]+', '', objetivosEspecificos)\n",
    "    \n",
    "    return objetivoGeneral, objetivosEspecificos\n",
    "\n",
    "def get_areas(documento):\n",
    "    directores = get_director(documento)\n",
    "    areas = [''] * len(directores)\n",
    "\n",
    "    for j, director in enumerate(directores):\n",
    "        try:\n",
    "            areas[j] = re.split(director, documento, flags=re.IGNORECASE | re.MULTILINE)[2]\n",
    "        except IndexError:\n",
    "            try:\n",
    "                director_sin_acentos = re.sub(r'[áéíóú]', lambda m: m.group(0).lower(), director)\n",
    "                areas[j] = re.split(director_sin_acentos, documento, flags=re.IGNORECASE | re.MULTILINE)[1]\n",
    "            except IndexError:\n",
    "                areas[j] = '-'\n",
    "\n",
    "        if areas[j] != '-':\n",
    "            areas[j] = re.split(r\"Firma\", areas[j], flags=re.IGNORECASE | re.MULTILINE)[0]\n",
    "            if re.search(r'(?:Á|A)reas?\\s*(?:de)?\\s*inter(?:é|e)?s', areas[j], flags=re.IGNORECASE):\n",
    "                areas[j] = re.split(r'(?:Á|A)reas?\\s*(?:de)?\\s*inter(?:é|e)?s', areas[j], flags=re.IGNORECASE | re.MULTILINE)[1]\n",
    "                areas[j] = re.split(r\"\\.\", areas[j], maxsplit=1, flags=re.IGNORECASE | re.MULTILINE)[0]\n",
    "                areas[j] = re.sub(r'^[^a-zA-ZÁÉÍÓÚÜÑáéíóúüñ]+|\\s{2,}|\\n', ' ', areas[j]).strip()\n",
    "                areas[j] = [i.strip() for i in re.split(r\"\\,\", areas[j], flags=re.IGNORECASE | re.MULTILINE)]\n",
    "            else:\n",
    "                areas[j] = '-'\n",
    "    areas = [[re.sub(r'^[a-zÁÉÍÓÚÜÑáéíóúüñ]', lambda x: x.group(0).upper(), i) for i in area] for area in areas]\n",
    "    return areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_info(documento, i):\n",
    "    documento = eliminar_caracteres_especiales(documento)\n",
    "    titulo = get_title(documento)\n",
    "    autores = get_autores(documento)\n",
    "    ntt= get_ttnum(documento, pdf_files[i])\n",
    "    abstract = get_abstract(documento)\n",
    "    keywords = get_keywords(documento)\n",
    "    directores = get_director(documento)\n",
    "    areas = get_areas(documento)\n",
    "    objetivosG, objetivosE = get_objetivo(documento)\n",
    "    rutas = pdf_files[i]\n",
    "\n",
    "    return titulo, autores, ntt, directores, areas, keywords, abstract, objetivosG, objetivosE, rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 / 422\r"
     ]
    }
   ],
   "source": [
    "protocolos = []\n",
    "i=0\n",
    "for i in range(len(pdf_files)):\n",
    "    print(i + 1, '/', len(pdf_files), end='\\r')\n",
    "    protocolos.append(get_all_info(text[i], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir contenido de protocolos\n",
    "# for i in range(10):\n",
    "#     print(f\"Protocolo {i+1}\")\n",
    "#     print(f\"Titulo: {protocolos[i][0]}\")\n",
    "#     print(f\"Autores: {protocolos[i][1]}\")\n",
    "#     print(f\"Numero de TT: {protocolos[i][2]}\")\n",
    "#     print(f\"Directores: {protocolos[i][3]}\")\n",
    "#     print(f\"Areas: {protocolos[i][4]}\")\n",
    "#     print(f\"Keywords: {protocolos[i][5]}\")\n",
    "#     print(f\"Abstract: {protocolos[i][6]}\")\n",
    "#     print(f\"Objetivo General: {protocolos[i][7]}\")\n",
    "#     print(f\"Objetivos Especificos: {protocolos[i][8]}\")\n",
    "#     print(f\"Rutas:  {protocolos[i][9]}\")\n",
    "#     print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(protocolos, columns=['Título', 'Autores', 'Número de TT', 'Directores', 'Áreas', 'Palabras clave', 'Resumen', 'Objetivo general', 'Objetivos específicos', 'Ruta'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar df['Autores'] en un archivo xlsx\n",
    "df.to_excel('prueba2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel('protocolos.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m titulos_lema \u001b[39m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m tags \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mNOUN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVERB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mADJ\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mADV\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df_tt[\u001b[39m'\u001b[39m\u001b[39mtitulo\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     10\u001b[0m     doc \u001b[39m=\u001b[39m nlp(i)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# para cada token en el doc \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tt' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.es.examples import sentences \n",
    "# !python -m spacy download es_core_news_sm\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "titulos_lema = []\n",
    "tags = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "\n",
    "for i in df_tt['titulo']:\n",
    "    doc = nlp(i)\n",
    "    # para cada token en el doc \n",
    "    titulo_aux = ''\n",
    "    for token in doc:\n",
    "        # lematizar el token\n",
    "        if token.pos_ in tags:\n",
    "            titulo_aux += token.lemma_ + ' '\n",
    "    titulos_lema.append(titulo_aux)\n",
    "titulos_lema   \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIRECTORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROF</th>\n",
       "      <th>DEPTO</th>\n",
       "      <th>UNIDAD APRENDIZAJE</th>\n",
       "      <th>ACADEMIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aguilar García Rafael</td>\n",
       "      <td>CIC</td>\n",
       "      <td>Paradigmas de Programación</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aguilar García Rafael</td>\n",
       "      <td>CIC</td>\n",
       "      <td>Paradigmas de Programación</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aguilar Sánchez Fernando</td>\n",
       "      <td>ISC</td>\n",
       "      <td>Fundamentos de Diseño Digital</td>\n",
       "      <td>SDIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aguilar Sánchez Fernando</td>\n",
       "      <td>ISC</td>\n",
       "      <td>Introducción a los Microcontroladores</td>\n",
       "      <td>SDIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aguilar Sánchez Fernando</td>\n",
       "      <td>ISC</td>\n",
       "      <td>Fundamentos de Diseño Digital</td>\n",
       "      <td>SDIG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       PROF DEPTO                     UNIDAD APRENDIZAJE  \\\n",
       "0     Aguilar García Rafael   CIC             Paradigmas de Programación   \n",
       "1     Aguilar García Rafael   CIC             Paradigmas de Programación   \n",
       "2  Aguilar Sánchez Fernando   ISC          Fundamentos de Diseño Digital   \n",
       "3  Aguilar Sánchez Fernando   ISC  Introducción a los Microcontroladores   \n",
       "4  Aguilar Sánchez Fernando   ISC          Fundamentos de Diseño Digital   \n",
       "\n",
       "     ACADEMIA  \n",
       "0  CC          \n",
       "1  CC          \n",
       "2  SDIG        \n",
       "3  SDIG        \n",
       "4  SDIG        "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ABRIR XLSX CON NOMBRE Info TT 2023-B001.xlsx EN UN DATAFRAME\n",
    "df1 = pd.read_excel('Info TT 2023-B001.xlsx', sheet_name='21-2')\n",
    "df2 = pd.read_excel('Info TT 2023-B001.xlsx', sheet_name='22-1')\n",
    "df3 = pd.read_excel('Info TT 2023-B001.xlsx', sheet_name='22-2')\n",
    "df4 = pd.read_excel('Info TT 2023-B001.xlsx', sheet_name='23-1')\n",
    "\n",
    "# CONCATENAR LOS DATAFRAMES\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b350b301ab686cc38c89757293f17f33d256656378ea8ac36012022ad524cef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
