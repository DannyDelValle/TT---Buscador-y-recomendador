{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "\n",
    "path = os.getcwd()\n",
    "pdf_files = glob.glob(path + \"/Protocolos/*.pdf\")\n",
    "print(len(pdf_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso 1\n",
      "197/197\r"
     ]
    }
   ],
   "source": [
    "def convert_pdf_to_txt(path):\n",
    "    pdf_file = open(path, 'rb')\n",
    "    read_pdf = PyPDF2.PdfReader(pdf_file)\n",
    "    number_of_pages = len(read_pdf.pages)\n",
    "    page_content = \"\"\n",
    "    for page_number in range(number_of_pages):\n",
    "        page = read_pdf.pages[page_number]\n",
    "        page_content += page.extract_text()\n",
    "    pdf_file.close()\n",
    "    return page_content\n",
    "text = []\n",
    "index=1\n",
    "print(\"Proceso 1\")\n",
    "for i in pdf_files:\n",
    "    print(f\"{index}/{len(pdf_files)}\", end='\\r')\n",
    "    text.append(convert_pdf_to_txt(i))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(text):\n",
    "    text = text.split('trabajo terminal')\n",
    "    text = text[0]\n",
    "    text = text.replace('\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_ttnum(text):\n",
    "    text = text.split('trabajo terminal')\n",
    "    text = text[1]\n",
    "    # si alum aparece en ek texto\n",
    "    if 'alumn' in text:\n",
    "        text = text.split('alumn')\n",
    "        text = text[0]\n",
    "    if 'integrantes' in text:\n",
    "        text = text.split('integrantes')\n",
    "        text = text[0]\n",
    "    text = re.sub(' +', '', text)\n",
    "    # si aparece \"no.\" en text remover\n",
    "    if 'no.' in text:\n",
    "        text = text.replace('no.', '')\n",
    "    elif 'no:' in text:\n",
    "        text = text.replace('no:', '')\n",
    "    return text\n",
    "\n",
    "def get_director(text):\n",
    "    text = text.split('director')\n",
    "    text = text[1]\n",
    "    # si e-mail aparece en el texto \n",
    "    if 'e-mail' in text:\n",
    "        text = text.split('e-mail')\n",
    "        text = text[0]\n",
    "    elif 'resumen' in text:\n",
    "        text = text.split('resumen')\n",
    "        text = text[0]\n",
    "    # eliminar texto antes de :\n",
    "    text = text.split(':')\n",
    "    text = text[1]\n",
    "    #eliminar :\n",
    "    text = text.replace(':', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_abstract(text):\n",
    "    text = text.split('resumen')\n",
    "    text = text[1]\n",
    "    text = text.split('palabras clave')\n",
    "    text = text[0]\n",
    "    text = text.replace('\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_keywords(text):\n",
    "    text = text.split('palabras clave')\n",
    "    text = text[1]\n",
    "    text = text.split('.')\n",
    "    text = text[0]\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_areas(text):\n",
    "    # si existe areas de interes en el texto \n",
    "    if 'áreas de interés' in text:\n",
    "        text = text.split('áreas de interés')\n",
    "        text = text[1]\n",
    "        text = text.split('.')\n",
    "        text = text[0]\n",
    "        text = text.replace('\\n', '')\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        return text\n",
    "\n",
    "def get_objetivo(text):\n",
    "    text = text.split('objetivo')\n",
    "    text = text[1]\n",
    "    text = text.split('.')\n",
    "    text = text[0]\n",
    "    text = text.replace('\\n', '')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_objetivos_especificos(text):\n",
    "    # si existe \"objetivos especificos\" o \"especificos\" en el texto\n",
    "    if 'objetivos específicos' in text or 'específicos' in text:\n",
    "        if 'objetivos específicos' in text:\n",
    "            text = text.split('objetivos específicos')\n",
    "            text = text[1]\n",
    "            text = text.split('.')\n",
    "            text = text[0]\n",
    "            text = text.replace('\\n', '')\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            return text\n",
    "        else:\n",
    "            text = text.split('específicos')\n",
    "            text = text[1]\n",
    "            text = text.split('.')\n",
    "            text = text[0]\n",
    "            text = text.replace('\\n', '')\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasar todos los text a minusculas \n",
    "text = [x.lower() for x in text]\n",
    "# quitar todos los \"\\n\" de text\n",
    "text = [x.replace('\\n', '') for x in text]\n",
    "# quitar dobles espacios\n",
    "text = [re.sub(' +', ' ', x) for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso 2\n",
      "197/197\r"
     ]
    }
   ],
   "source": [
    "titulos = []\n",
    "num_tt = []\n",
    "abstracts = []\n",
    "keywords = []\n",
    "directores = []\n",
    "areas_interes = []\n",
    "objetivos = []\n",
    "objetivos_especificos=[]\n",
    "index=1\n",
    "print(\"Proceso 2\")\n",
    "for j in text:\n",
    "    print(f\"{index}/{len(pdf_files)}\", end='\\r')\n",
    "    titulos.append(get_title(j))\n",
    "    # print(f\"{titulos[index-1]}\", end='\\r')\n",
    "    num_tt.append(get_ttnum(j))\n",
    "    abstracts.append(get_abstract(j))\n",
    "    keywords.append(get_keywords(j))\n",
    "    directores.append(get_director(j))\n",
    "    areas_interes.append(get_areas(j))\n",
    "    objetivos.append(get_objetivo(j))\n",
    "    objetivos_especificos.append(get_objetivos_especificos(j))\n",
    "    index+=1\n",
    "# obtener Objetivos específicos: hasta Justificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' dr. suárez castañón miguel santiago, dr. carlos alberto duchanoy martínez,turno para la presentación del tt',\n",
       " ' vélez saldaña ulises , jiménez galán yasmín ivette *montes.romero@hotmail.com , sergalanm@gmail.com ',\n",
       " ' m. en c. zagal flores roberto eswart, dr. mata rivera miguel félix turno para la presentación del tt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>N_TT</th>\n",
       "      <th>Directores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prototipo de aplicación móvil para temas de f...</td>\n",
       "      <td>2019-b037</td>\n",
       "      <td>dra. ruíz ledesma elena fabiola *ocampos97@ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 2 framework para la implementación de interf...</td>\n",
       "      <td>2019-a053</td>\n",
       "      <td>pescador rojas miriam, coronilla contreras uk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>monedero electronico para cafeterias escolares...</td>\n",
       "      <td>____-___</td>\n",
       "      <td>dr. amadeo josé argüelles cruz, m. en ing. al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 herramienta para la creación de un entorno p...</td>\n",
       "      <td>————-———</td>\n",
       "      <td>carreto arellano chadwick , bustos farías edu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 software de evaluación de datos de espectrom...</td>\n",
       "      <td>2016-b015</td>\n",
       "      <td>dr. tonáhtiu arturo ramírez romero , miguel h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titulo       N_TT  \\\n",
       "0   prototipo de aplicación móvil para temas de f...  2019-b037   \n",
       "1  1 2 framework para la implementación de interf...  2019-a053   \n",
       "2  monedero electronico para cafeterias escolares...   ____-___   \n",
       "3  1 herramienta para la creación de un entorno p...   ————-———   \n",
       "4  1 software de evaluación de datos de espectrom...  2016-b015   \n",
       "\n",
       "                                          Directores  \n",
       "0   dra. ruíz ledesma elena fabiola *ocampos97@ou...  \n",
       "1   pescador rojas miriam, coronilla contreras uk...  \n",
       "2   dr. amadeo josé argüelles cruz, m. en ing. al...  \n",
       "3   carreto arellano chadwick , bustos farías edu...  \n",
       "4   dr. tonáhtiu arturo ramírez romero , miguel h...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar en un archivo de excel con las columnas titulo, N_TT, abstract, keywords, directores, objetivos\n",
    "df = pd.DataFrame({'titulo': titulos , 'N_TT': num_tt,'Directores': directores})\n",
    "\n",
    "# save the dataframe in a excel with the name \"extraccion\"\n",
    "df.to_excel('extraccion.xlsx', index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexamples\u001b[39;00m \u001b[39mimport\u001b[39;00m sentences \n\u001b[1;32m----> 4\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mes_core_news_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39m# para cada titulo de los titulos procesar con nlp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mtitulo\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\jddel\\miniconda3\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jddel\\miniconda3\\lib\\site-packages\\spacy\\util.py:439\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# procesar df con spacy \n",
    "import spacy\n",
    "from spacy.lang.es.examples import sentences \n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "# how to install spacy \n",
    "# https://spacy.io/usage/models#languages\n",
    "\n",
    "\n",
    "# para cada titulo de los titulos procesar con nlp\n",
    "for i in df['titulo']:\n",
    "    doc = nlp(i)\n",
    "    # para cada token en el doc \n",
    "    for token in doc:\n",
    "        # si el token es un verbo \n",
    "        if token.pos_ == 'VERB':\n",
    "            print(token.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b350b301ab686cc38c89757293f17f33d256656378ea8ac36012022ad524cef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
